---
title: Building a Chatbot with BAML React Hooks
description: Learn to build a streaming chatbot using BAML React hooks and Next.js
---

In this tutorial, you'll build a real-time streaming chatbot using BAML React hooks. By following along, you'll learn how to:
- Create a BAML function for chat completions
- Use BAML's React hooks for streaming responses
- Build a modern chat interface
- Handle loading states and errors

## Prerequisites

Before starting, ensure you have:
- Completed the [Quick Start Guide](/guide/framework-integration/react-next-js/quick-start)
- A Next.js project with BAML set up
- An OpenAI API key

## Step 1: Define the Chat Function

First, create a new BAML function for the chat completion:

<CodeBlocks>
```baml title="baml_src/chat.baml"
class Message {
  role: string
  content: string
}

function Chat(messages: Message[]) -> Message {
  client openai/gpt-4o-mini
  prompt #"
    You are a helpful and knowledgeable AI assistant engaging in a conversation.
    Your responses should be:
    - Clear and concise
    - Accurate and informative
    - Natural and conversational in tone
    - Focused on addressing the user's needs

    {{ _.role("user" )}}
    Previous conversation:
    {% for message in messages %}
    {{message.role}}: {{message.content}}
    {% endfor %}

    assistant:
  "#
}
```
</CodeBlocks>

Generate the BAML client to create the React hooks:

```bash
baml-cli generate
```

## Step 2: Implement the Chat Interface

You can implement the chat interface in two ways:

### Option A: Using the Generated Hook Directly

The simplest approach is to use the generated hook directly:

<CodeBlocks>
```tsx title="app/components/chat-interface.tsx"
'use client'

import { useChat } from "@/baml_client/react/hooks";
import { useState } from "react";

export function ChatInterface() {
  const [input, setInput] = useState("");

  const {
    mutate: sendMessage,
    isPending,
    data: messages,
    error
  } = useChat();

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim()) return;

    const newMessages = [
      ...messages,
      { role: "user", content: input }
    ];
    setInput("");

    try {
      await sendMessage({ messages: newMessages });
    } catch (error) {
      console.error("Chat error:", error);
    }
  };

  return (
    <div>
      <div>
        {messages.map((message, i) => (
          <div key={i}>
            {message.content}
          </div>
        ))}
        {isPending && (
          <div>Typing...</div>
        )}
      </div>

      <form onSubmit={handleSubmit}>
        <div>
          <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            placeholder="Type your message..."
            disabled={isPending}
          />
          <button type="submit" disabled={isPending}>
            Send
          </button>
        </div>
      </form>
    </div>
  );
}
```
</CodeBlocks>

### Option B: Using a Custom Server Action

Alternatively, you can create a custom server action for more control over the server-side implementation:

<CodeBlocks>
```ts title="app/actions/chat.ts"
'use server'

import { b } from "@/baml_client";
import { Message } from "@/baml_client/types";

export async function streamChat(messages: Message[]) {
  const user = await authUser();

  if (!user) {
    throw new Error("User not authenticated");
  }

  return b.stream.Chat(messages).toStreamable();
}
```

```tsx title="app/components/chat-interface-with-action.tsx"
'use client'

import { useChat } from "@/baml_client/react/hooks";
import { streamChat } from "../actions/chat";
import { useState } from "react";

export function ChatInterface() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState("");
  const [isPending, setIsPending] = useState(false);
  const [error, setError] = useState<Error | null>(null);

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim()) return;

    const newMessages = [
      ...messages,
      { role: "user", content: input }
    ];
    setInput("");
    setIsPending(true);
    setError(null);

    try {
      const stream = await streamChat(newMessages);

      for await (const message of stream) {
        setMessages((prev) => [...prev, message]);
      }
    } catch (error) {
      setError(error as Error);
    } finally {
      setIsPending(false);
    }
  };

  return (
    <div>
      <div>
        {messages.map((message, i) => (
          <div key={i}>
            {message.content}
          </div>
        ))}
        {isPending && (
          <div>Typing...</div>
        )}
      </div>

      <form onSubmit={handleSubmit}>
        <div>
          <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            placeholder="Type your message..."
            disabled={isPending}
          />
          <button type="submit" disabled={isPending}>
            Send
          </button>
        </div>
      </form>
    </div>
  );
}
```
</CodeBlocks>

The server action approach is useful when you need to:
- Add custom server-side logic
- Handle authentication
- Add logging or monitoring
- Implement rate limiting
- Add custom error handling

## Next Steps

To enhance your chatbot, you could:
- Add [error handling](/ref/baml_client/errors/overview) for different types of errors
- Add chat history persistence
- Implement different chat models or configurations

For more information, check out:
- [Generated Hooks](/ref/baml_client/react-next-js/use-function-name-hook)
- [HookInput](/ref/baml_client/react-next-js/hook-input)
- [HookOutput](/ref/baml_client/react-next-js/hook-output)
- [Error Handling](/ref/baml_client/errors/overview)
