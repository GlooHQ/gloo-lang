---
title: Prompting in BAML
---

<Note>
We recommend reading the [installation](/) instructions first
</Note>

BAML functions are special definitions that get converted into real code (Python, TS, etc) that calls LLMs. Think of them as a way to define AI-powered functions that are type-safe and easy to use in your application.

### What BAML Functions Actually Do
When you write a BAML function like this:

```rust BAML
function ExtractResume(resume_text: string) -> Resume {
  client "openai/gpt-4o"
  // The prompt uses Jinja syntax.. more on this soon.
  prompt #"
     Extract info from this text.

    {{ ctx.output_format }}

    Resume:
    ---
    {{ resume_text }}
    ---
  "#
}
```

BAML converts it into code that:

1. Takes your input (`resume_text`)
2. Sends a request to OpenAI's GPT-4 API with your prompt
3. Parses the JSON response into your `Resume` type
4. Returns a type-safe object you can use in your code

If you are using VSCode extension, you can see the how the prompt is rendered, and what `ctx.output_format` is:

<img src="/assets/vscode/extract-resume-prompt-preview.png" alt="Prompt preview" />

And you can dive even deeper to see the **Raw CURL request** BAML will make to the LLM provider:

<img src="/assets/vscode/curl-preview.png" alt="Raw CURL request" />

<Warning>
Always include the `{{ ctx.output_format }}` macro in your prompt. This injects your output schema into the prompt, which helps the LLM output the right thing. You can also [customize what it prints](/reference).

One of our design philosophies is to never hide the prompt from you. You control and can always see the entire prompt.
</Warning>

## Calling the function
Recall that BAML will generate a `baml_client` directory in the language of your choice using the parameters in your [`generator`](/ref/baml/generator) config. 

Here's a sample project structure with the `baml_client` directory:
<img src="/assets/vscode/baml-client.png" alt="Generated client" width={300} />

Now we can call the function, which will make a request to the LLM and return the `Resume` object:
<CodeBlocks>
```python python
from baml_client import b
from baml_client.types import Resume

def main():
resume_text = """Jason Doe\nPython, Rust\nUniversity of California, Berkeley, B.S.\nin Computer Science, 2020\nAlso an expert in Tableau, SQL, and C++\n"""

    # this function comes from the autogenerated "baml_client".
    # It calls the LLM you specified and handles the parsing.
    resume = b.ExtractResume(resume_text)

    # Fully type-checked and validated!
    assert isinstance(resume, Resume)

```

```typescript typescript
import b from 'baml_client'

async function main() {
  const resume_text = `Jason Doe\nPython, Rust\nUniversity of California, Berkeley, B.S.\nin Computer Science, 2020\nAlso an expert in Tableau, SQL, and C++`

  // this function comes from the autogenerated "baml_client".
  // It calls the LLM you specified and handles the parsing.
  const resume = await b.ExtractResume(resume_text)

  // Fully type-checked and validated!
  resume.name === 'Jason Doe'
}
```

```ruby ruby

require_relative "baml_client/client"
b = Baml.Client

# Note this is not async
res = b.TestFnNamedArgsSingleClass(
    myArg: Baml::Types::Resume.new(
        key: "key",
        key_two: true,
        key_three: 52,
    )
)
```

</CodeBlocks>


<Warning>
Do not modify any code inside `baml_client`, as it's autogenerated.
</Warning>

## Next steps

Checkout [PromptFiddle](https://promptfiddle.com) to see various interactive BAML function examples or view the [example projects](/examples)

Read the next guide to learn more about choosing different LLM providers and running tests in the VSCode extension.